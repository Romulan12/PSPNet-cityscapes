{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSPNetTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyW8cYgG7vDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import imageio\n",
        "import math\n",
        "import numbers\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision.transforms.functional as tf\n",
        "from skimage.transform import resize as re\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.misc as m\n",
        "\n",
        "from torch.utils import data\n",
        "import caffe_pb2\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from utils import conv2DBatchNormRelu, residualBlockPSP, pyramidPooling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp69CXV_8K0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_scale_cross_entropy2d(input, target, weight=None, size_average=True, scale_weight=None):\n",
        "    if not isinstance(input, tuple):\n",
        "        return cross_entropy2d(input=input, target=target, weight=weight, size_average=size_average)\n",
        "\n",
        "    # Auxiliary training for PSPNet [1.0, 0.4] and ICNet [1.0, 0.4, 0.16]\n",
        "    if scale_weight is None:  # scale_weight: torch tensor type\n",
        "        n_inp = len(input)\n",
        "        scale = 0.4\n",
        "        scale_weight = torch.pow(scale * torch.ones(n_inp), torch.arange(n_inp).float()).to(\n",
        "            target.device\n",
        "        )\n",
        "\n",
        "    loss = 0.0\n",
        "    for i, inp in enumerate(input):\n",
        "        loss = loss + scale_weight[i] * cross_entropy2d(\n",
        "            input=inp, target=target, weight=weight, size_average=size_average\n",
        "        )\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCHqp70k9Z6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.misc as m\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "class cityscapeDatasetLoad(data.Dataset):\n",
        "    colors = [  # [  0,   0,   0],\n",
        "        [128, 64, 128],\n",
        "        [244, 35, 232],\n",
        "        [70, 70, 70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170, 30],\n",
        "        [220, 220, 0],\n",
        "        [107, 142, 35],\n",
        "        [152, 251, 152],\n",
        "        [0, 130, 180],\n",
        "        [220, 20, 60],\n",
        "        [255, 0, 0],\n",
        "        [0, 0, 142],\n",
        "        [0, 0, 70],\n",
        "        [0, 60, 100],\n",
        "        [0, 80, 100],\n",
        "        [0, 0, 230],\n",
        "        [119, 11, 32],\n",
        "    ]\n",
        "    label_colours = dict(zip(range(19), colors))\n",
        "    mean_rgb = [0.0, 0.0, 0.0]\n",
        "\n",
        "    def __init__(self, root, is_transform= True, img_size = (2048,1024), img_norm = True, version = \"cityscapes\"):\n",
        "        self.root = root\n",
        "        self.is_transform = is_transform\n",
        "        self.n_classes = 19\n",
        "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
        "        self.mean = np.array(self.mean_rgb)\n",
        "        self.files = {}\n",
        "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
        "        self.valid_classes = [\n",
        "            7,\n",
        "            8,\n",
        "            11,\n",
        "            12,\n",
        "            13,\n",
        "            17,\n",
        "            19,\n",
        "            20,\n",
        "            21,\n",
        "            22,\n",
        "            23,\n",
        "            24,\n",
        "            25,\n",
        "            26,\n",
        "            27,\n",
        "            28,\n",
        "            31,\n",
        "            32,\n",
        "            33,\n",
        "        ]\n",
        "        self.class_names = [\n",
        "            \"unlabelled\",\n",
        "            \"road\",\n",
        "            \"sidewalk\",\n",
        "            \"building\",\n",
        "            \"wall\",\n",
        "            \"fence\",\n",
        "            \"pole\",\n",
        "            \"traffic_light\",\n",
        "            \"traffic_sign\",\n",
        "            \"vegetation\",\n",
        "            \"terrain\",\n",
        "            \"sky\",\n",
        "            \"person\",\n",
        "            \"rider\",\n",
        "            \"car\",\n",
        "            \"truck\",\n",
        "            \"bus\",\n",
        "            \"train\",\n",
        "            \"motorcycle\",\n",
        "            \"bicycle\",\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = imageio.imread(img_path)\n",
        "        img = np.array(img, dtype = np.uint8)\n",
        "\n",
        "        if self.is_transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def transform(self, img):\n",
        "        img = np.array\n",
        "\n",
        "        img = np.array(Image.fromarray(img).resize((self.img_size[0], self.img_size[1]), resample = 0))\n",
        "                #img = m.imresize(img, (self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\n",
        "        img = img[:, :, ::-1]  # RGB -> BGR\n",
        "        img = img.astype(np.float64)\n",
        "        img -= self.mean\n",
        "        if self.img_norm:\n",
        "            # Resize scales images from 0 to 255, thus we need\n",
        "            # to divide by 255.0\n",
        "            img = img.astype(float) / 255.0\n",
        "        # NHWC -> NCHW\n",
        "        img = img.transpose(2, 0, 1)\n",
        "        img = torch.from_numpy(img).float()\n",
        "\n",
        "        return img\n",
        "\n",
        "    def decode_segmap(self, temp):\n",
        "        r = temp.copy()\n",
        "        g = temp.copy()\n",
        "        b = temp.copy()\n",
        "        for l in range(0, self.n_classes):\n",
        "            r[temp == l] = self.label_colours[l][0]\n",
        "            g[temp == l] = self.label_colours[l][1]\n",
        "            b[temp == l] = self.label_colours[l][2]\n",
        "\n",
        "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
        "        rgb[:, :, 0] = r / 255.0\n",
        "        rgb[:, :, 1] = g / 255.0\n",
        "        rgb[:, :, 2] = b / 255.0\n",
        "        return rgb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFLhkvvR_YjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pspnet_specs = {\"cityscapes\": {\"n_classes\": 19, \"input_size\": (713, 713), \"block_config\": [3, 4, 23, 3]}}\n",
        "\n",
        "\n",
        "class pspnet(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self, n_classes=19, block_config=[3, 4, 23, 3], input_size=(473, 473), version=None\n",
        "    ):\n",
        "\n",
        "        super(pspnet, self).__init__()\n",
        "\n",
        "        self.block_config = (\n",
        "            pspnet_specs[version][\"block_config\"] if version is not None else block_config\n",
        "        )\n",
        "        self.n_classes = pspnet_specs[version][\"n_classes\"] if version is not None else n_classes\n",
        "        self.input_size = pspnet_specs[version][\"input_size\"] if version is not None else input_size\n",
        "\n",
        "        # Encoder\n",
        "        self.convbnrelu1_1 = conv2DBatchNormRelu(\n",
        "            in_channels=3, k_size=3, n_filters=64, padding=1, stride=2, bias=False\n",
        "        )\n",
        "        self.convbnrelu1_2 = conv2DBatchNormRelu(\n",
        "            in_channels=64, k_size=3, n_filters=64, padding=1, stride=1, bias=False\n",
        "        )\n",
        "        self.convbnrelu1_3 = conv2DBatchNormRelu(\n",
        "            in_channels=64, k_size=3, n_filters=128, padding=1, stride=1, bias=False\n",
        "        )\n",
        "\n",
        "        # Vanilla Residual Blocks\n",
        "        self.res_block2 = residualBlockPSP(self.block_config[0], 128, 64, 256, 1, 1)\n",
        "        self.res_block3 = residualBlockPSP(self.block_config[1], 256, 128, 512, 2, 1)\n",
        "\n",
        "        # Dilated Residual Blocks\n",
        "        self.res_block4 = residualBlockPSP(self.block_config[2], 512, 256, 1024, 1, 2)\n",
        "        self.res_block5 = residualBlockPSP(self.block_config[3], 1024, 512, 2048, 1, 4)\n",
        "\n",
        "        # Pyramid Pooling Module\n",
        "        self.pyramid_pooling = pyramidPooling(2048, [6, 3, 2, 1])\n",
        "\n",
        "        # Final conv layers\n",
        "        self.cbr_final = conv2DBatchNormRelu(4096, 512, 3, 1, 1, False)\n",
        "        self.dropout = nn.Dropout2d(p=0.1, inplace=False)\n",
        "        self.classification = nn.Conv2d(512, self.n_classes, 1, 1, 0)\n",
        "\n",
        "        # Auxiliary layers for training\n",
        "        self.convbnrelu4_aux = conv2DBatchNormRelu(\n",
        "            in_channels=1024, k_size=3, n_filters=256, padding=1, stride=1, bias=False\n",
        "        )\n",
        "        self.aux_cls = nn.Conv2d(256, self.n_classes, 1, 1, 0)\n",
        "\n",
        "        # Define auxiliary loss function\n",
        "        self.loss = multi_scale_cross_entropy2d\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp_shape = x.shape[2:]\n",
        "\n",
        "        # H, W -> H/2, W/2\n",
        "        x = self.convbnrelu1_1(x)\n",
        "        x = self.convbnrelu1_2(x)\n",
        "        x = self.convbnrelu1_3(x)\n",
        "\n",
        "        # H/2, W/2 -> H/4, W/4\n",
        "        x = F.max_pool2d(x, 3, 2, 1)\n",
        "\n",
        "        # H/4, W/4 -> H/8, W/8\n",
        "        x = self.res_block2(x)\n",
        "        x = self.res_block3(x)\n",
        "        x = self.res_block4(x)\n",
        "\n",
        "        # Auxiliary layers for training\n",
        "        if self.training:\n",
        "            x_aux = self.convbnrelu4_aux(x)\n",
        "            x_aux = self.dropout(x_aux)\n",
        "            x_aux = self.aux_cls(x_aux)\n",
        "\n",
        "        x = self.res_block5(x)\n",
        "\n",
        "        x = self.pyramid_pooling(x)\n",
        "\n",
        "        x = self.cbr_final(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.classification(x)\n",
        "        x = F.interpolate(x, size=inp_shape, mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        if self.training:\n",
        "            return (x, x_aux)\n",
        "        else:  # eval mode\n",
        "            return x\n",
        "\n",
        "    def load_pretrained_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Load weights from caffemodel w/o caffe dependency\n",
        "        and plug them in corresponding modules\n",
        "        \"\"\"\n",
        "        # My eyes and my heart both hurt when writing this method\n",
        "\n",
        "        # Only care about layer_types that have trainable parameters\n",
        "        ltypes = [\"BNData\", \"ConvolutionData\", \"HoleConvolutionData\"]\n",
        "\n",
        "        def _get_layer_params(layer, ltype):\n",
        "\n",
        "            if ltype == \"BNData\":\n",
        "                gamma = np.array(layer.blobs[0].data)\n",
        "                beta = np.array(layer.blobs[1].data)\n",
        "                mean = np.array(layer.blobs[2].data)\n",
        "                var = np.array(layer.blobs[3].data)\n",
        "                return [mean, var, gamma, beta]\n",
        "\n",
        "            elif ltype in [\"ConvolutionData\", \"HoleConvolutionData\"]:\n",
        "                is_bias = layer.convolution_param.bias_term\n",
        "                weights = np.array(layer.blobs[0].data)\n",
        "                bias = []\n",
        "                if is_bias:\n",
        "                    bias = np.array(layer.blobs[1].data)\n",
        "                return [weights, bias]\n",
        "\n",
        "            elif ltype == \"InnerProduct\":\n",
        "                raise Exception(\"Fully connected layers {}, not supported\".format(ltype))\n",
        "\n",
        "            else:\n",
        "                raise Exception(\"Unkown layer type {}\".format(ltype))\n",
        "\n",
        "        net = caffe_pb2.NetParameter()\n",
        "        with open(model_path, \"rb\") as model_file:\n",
        "            net.MergeFromString(model_file.read())\n",
        "\n",
        "        # dict formatted as ->  key:<layer_name> :: value:<layer_type>\n",
        "        layer_types = {}\n",
        "        # dict formatted as ->  key:<layer_name> :: value:[<list_of_params>]\n",
        "        layer_params = {}\n",
        "\n",
        "        for l in net.layer:\n",
        "            lname = l.name\n",
        "            ltype = l.type\n",
        "            if ltype in ltypes:\n",
        "                print(\"Processing layer {}\".format(lname))\n",
        "                layer_types[lname] = ltype\n",
        "                layer_params[lname] = _get_layer_params(l, ltype)\n",
        "\n",
        "        # Set affine=False for all batchnorm modules\n",
        "        def _no_affine_bn(module=None):\n",
        "            if isinstance(module, nn.BatchNorm2d):\n",
        "                module.affine = False\n",
        "\n",
        "            if len([m for m in module.children()]) > 0:\n",
        "                for child in module.children():\n",
        "                    _no_affine_bn(child)\n",
        "\n",
        "        # _no_affine_bn(self)\n",
        "\n",
        "        def _transfer_conv(layer_name, module):\n",
        "            weights, bias = layer_params[layer_name]\n",
        "            w_shape = np.array(module.weight.size())\n",
        "\n",
        "            print(\n",
        "                \"CONV {}: Original {} and trans weights {}\".format(\n",
        "                    layer_name, w_shape, weights.shape\n",
        "                )\n",
        "            )\n",
        "\n",
        "            module.weight.data.copy_(torch.from_numpy(weights).view_as(module.weight))\n",
        "\n",
        "            if len(bias) != 0:\n",
        "                b_shape = np.array(module.bias.size())\n",
        "                print(\n",
        "                    \"CONV {}: Original {} and trans bias {}\".format(layer_name, b_shape, bias.shape)\n",
        "                )\n",
        "                module.bias.data.copy_(torch.from_numpy(bias).view_as(module.bias))\n",
        "\n",
        "        def _transfer_conv_bn(conv_layer_name, mother_module):\n",
        "            conv_module = mother_module[0]\n",
        "            bn_module = mother_module[1]\n",
        "\n",
        "            _transfer_conv(conv_layer_name, conv_module)\n",
        "\n",
        "            mean, var, gamma, beta = layer_params[conv_layer_name + \"/bn\"]\n",
        "            print(\n",
        "                \"BN {}: Original {} and trans weights {}\".format(\n",
        "                    conv_layer_name, bn_module.running_mean.size(), mean.shape\n",
        "                )\n",
        "            )\n",
        "            bn_module.running_mean.copy_(torch.from_numpy(mean).view_as(bn_module.running_mean))\n",
        "            bn_module.running_var.copy_(torch.from_numpy(var).view_as(bn_module.running_var))\n",
        "            bn_module.weight.data.copy_(torch.from_numpy(gamma).view_as(bn_module.weight))\n",
        "            bn_module.bias.data.copy_(torch.from_numpy(beta).view_as(bn_module.bias))\n",
        "\n",
        "        def _transfer_residual(prefix, block):\n",
        "            block_module, n_layers = block[0], block[1]\n",
        "\n",
        "            bottleneck = block_module.layers[0]\n",
        "            bottleneck_conv_bn_dic = {\n",
        "                prefix + \"_1_1x1_reduce\": bottleneck.cbr1.cbr_unit,\n",
        "                prefix + \"_1_3x3\": bottleneck.cbr2.cbr_unit,\n",
        "                prefix + \"_1_1x1_proj\": bottleneck.cb4.cb_unit,\n",
        "                prefix + \"_1_1x1_increase\": bottleneck.cb3.cb_unit,\n",
        "            }\n",
        "\n",
        "            for k, v in bottleneck_conv_bn_dic.items():\n",
        "                _transfer_conv_bn(k, v)\n",
        "\n",
        "            for layer_idx in range(2, n_layers + 1):\n",
        "                residual_layer = block_module.layers[layer_idx - 1]\n",
        "                residual_conv_bn_dic = {\n",
        "                    \"_\".join(\n",
        "                        map(str, [prefix, layer_idx, \"1x1_reduce\"])\n",
        "                    ): residual_layer.cbr1.cbr_unit,\n",
        "                    \"_\".join(map(str, [prefix, layer_idx, \"3x3\"])): residual_layer.cbr2.cbr_unit,\n",
        "                    \"_\".join(\n",
        "                        map(str, [prefix, layer_idx, \"1x1_increase\"])\n",
        "                    ): residual_layer.cb3.cb_unit,\n",
        "                }\n",
        "\n",
        "                for k, v in residual_conv_bn_dic.items():\n",
        "                    _transfer_conv_bn(k, v)\n",
        "\n",
        "        convbn_layer_mapping = {\n",
        "            \"conv1_1_3x3_s2\": self.convbnrelu1_1.cbr_unit,\n",
        "            \"conv1_2_3x3\": self.convbnrelu1_2.cbr_unit,\n",
        "            \"conv1_3_3x3\": self.convbnrelu1_3.cbr_unit,\n",
        "            \"conv5_3_pool6_conv\": self.pyramid_pooling.paths[0].cbr_unit,\n",
        "            \"conv5_3_pool3_conv\": self.pyramid_pooling.paths[1].cbr_unit,\n",
        "            \"conv5_3_pool2_conv\": self.pyramid_pooling.paths[2].cbr_unit,\n",
        "            \"conv5_3_pool1_conv\": self.pyramid_pooling.paths[3].cbr_unit,\n",
        "            \"conv5_4\": self.cbr_final.cbr_unit,\n",
        "            \"conv4_\" + str(self.block_config[2] + 1): self.convbnrelu4_aux.cbr_unit,\n",
        "        }  # Auxiliary layers for training\n",
        "\n",
        "        residual_layers = {\n",
        "            \"conv2\": [self.res_block2, self.block_config[0]],\n",
        "            \"conv3\": [self.res_block3, self.block_config[1]],\n",
        "            \"conv4\": [self.res_block4, self.block_config[2]],\n",
        "            \"conv5\": [self.res_block5, self.block_config[3]],\n",
        "        }\n",
        "\n",
        "        # Transfer weights for all non-residual conv+bn layers\n",
        "        for k, v in convbn_layer_mapping.items():\n",
        "            _transfer_conv_bn(k, v)\n",
        "\n",
        "        # Transfer weights for final non-bn conv layer\n",
        "        _transfer_conv(\"conv6\", self.classification)\n",
        "        _transfer_conv(\"conv6_1\", self.aux_cls)\n",
        "\n",
        "        # Transfer weights for all residual layers\n",
        "        for k, v in residual_layers.items():\n",
        "            _transfer_residual(k, v)\n",
        "\n",
        "    def tile_predict(self, imgs, include_flip_mode=True):\n",
        "        \"\"\"\n",
        "        Predict by takin overlapping tiles from the image.\n",
        "        Strides are adaptively computed from the imgs shape\n",
        "        and input size\n",
        "        :param imgs: torch.Tensor with shape [N, C, H, W] in BGR format\n",
        "        :param side: int with side length of model input\n",
        "        :param n_classes: int with number of classes in seg output.\n",
        "        \"\"\"\n",
        "\n",
        "        side_x, side_y = self.input_size\n",
        "        n_classes = self.n_classes\n",
        "        n_samples, c, h, w = imgs.shape\n",
        "        # n = int(max(h,w) / float(side) + 1)\n",
        "        n_x = int(h / float(side_x) + 1)\n",
        "        n_y = int(w / float(side_y) + 1)\n",
        "        stride_x = (h - side_x) / float(n_x)\n",
        "        stride_y = (w - side_y) / float(n_y)\n",
        "\n",
        "        x_ends = [[int(i * stride_x), int(i * stride_x) + side_x] for i in range(n_x + 1)]\n",
        "        y_ends = [[int(i * stride_y), int(i * stride_y) + side_y] for i in range(n_y + 1)]\n",
        "\n",
        "        pred = np.zeros([n_samples, n_classes, h, w])\n",
        "        count = np.zeros([h, w])\n",
        "\n",
        "        slice_count = 0\n",
        "        for sx, ex in x_ends:\n",
        "            for sy, ey in y_ends:\n",
        "                slice_count += 1\n",
        "\n",
        "                imgs_slice = imgs[:, :, sx:ex, sy:ey]\n",
        "                if include_flip_mode:\n",
        "                    imgs_slice_flip = torch.from_numpy(\n",
        "                        np.copy(imgs_slice.cpu().numpy()[:, :, :, ::-1])\n",
        "                    ).float()\n",
        "\n",
        "                is_model_on_cuda = next(self.parameters()).is_cuda\n",
        "\n",
        "                inp = Variable(imgs_slice, volatile=True)\n",
        "                if include_flip_mode:\n",
        "                    flp = Variable(imgs_slice_flip, volatile=True)\n",
        "\n",
        "                if is_model_on_cuda:\n",
        "                    inp = inp.cuda()\n",
        "                    if include_flip_mode:\n",
        "                        flp = flp.cuda()\n",
        "\n",
        "                psub1 = F.softmax(self.forward(inp), dim=1).data.cpu().numpy()\n",
        "                if include_flip_mode:\n",
        "                    psub2 = F.softmax(self.forward(flp), dim=1).data.cpu().numpy()\n",
        "                    psub = (psub1 + psub2[:, :, :, ::-1]) / 2.0\n",
        "                else:\n",
        "                    psub = psub1\n",
        "\n",
        "                pred[:, :, sx:ex, sy:ey] = psub\n",
        "                count[sx:ex, sy:ey] += 1.0\n",
        "\n",
        "        score = (pred / count[None, None, ...]).astype(np.float32)\n",
        "        return score / np.expand_dims(score.sum(axis=1), axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NG_ZOI69vWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import scipy.misc as misc\n",
        "\n",
        "#from data_loader import cityscapesLoaderTEST\n",
        "\n",
        "\n",
        "def test():\n",
        "    cd = 0 \n",
        "    import os\n",
        "    import scipy.misc as m\n",
        "\n",
        "    model = pspnet(version = \"cityscapes\")\n",
        "    caffemodel_dir_path = \"/content/drive/My Drive/cityscapes/model/PSP/\"\n",
        "      \n",
        "    model.load_pretrained_model(\n",
        "          model_path=os.path.join(caffemodel_dir_path, \"pspnet101_cityscapes.caffemodel\")\n",
        "      )\n",
        "\n",
        "    #model.load_state_dict(torch.load('/content/drive/My Drive/cityscapes/model/PSP/pspnet_101_cityscapes.pth', map_location= 'cpu'))\n",
        "\n",
        "    model.float()\n",
        "    model.cuda(cd)\n",
        "    with torch.no_grad():\n",
        "    \tmodel.eval()\n",
        "    out_path = \"/content/drive/My Drive/cityscapes/model/PSP/RESULTS/\"\n",
        "    dataset_root_dir = \"/content/drive/My Drive/cityscapes/\"\n",
        "    loader = cityscapeDatasetLoad(root = dataset_root_dir)\n",
        "    dst = loader\n",
        "    test_dirs = ['bonn/']\n",
        "    for t in test_dirs:\n",
        "      test_path = dataset_root_dir + '/leftImg8bit/test/' + t\n",
        "      file_names = os.listdir(test_path)\n",
        "      for f in file_names: \n",
        "        img_path = test_path + f\n",
        "        \n",
        "\n",
        "        img = imageio.imread(img_path)\n",
        "        original_size = img.shape[:-1]\n",
        "        img = img.transpose(2,0,1)\n",
        "        img = img.astype(np.float64)\n",
        "        img -=np.array([123.68, 116.779, 103.939])[:, None, None]\n",
        "        img = np.copy(img[::-1,:,:])\n",
        "        img = torch.from_numpy(img).float()    #convert to torch tensor\n",
        "        img = img.unsqueeze(0)\n",
        "        out = model.tile_predict(img)\n",
        "        pred = np.argmax(out, axis =1)[0]\n",
        "        decoded = dst.decode_segmap(pred)\n",
        "        imageio.imwrite(out_path + f, decoded)\n",
        "      print('file {f} saved at', out_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpGcMnNV-eoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf274779-2862-4ed4-c993-17b643456fd8"
      },
      "source": [
        "test()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing layer conv1_1_3x3_s2\n",
            "Processing layer conv1_1_3x3_s2/bn\n",
            "Processing layer conv1_2_3x3\n",
            "Processing layer conv1_2_3x3/bn\n",
            "Processing layer conv1_3_3x3\n",
            "Processing layer conv1_3_3x3/bn\n",
            "Processing layer conv2_1_1x1_reduce\n",
            "Processing layer conv2_1_1x1_reduce/bn\n",
            "Processing layer conv2_1_3x3\n",
            "Processing layer conv2_1_3x3/bn\n",
            "Processing layer conv2_1_1x1_increase\n",
            "Processing layer conv2_1_1x1_increase/bn\n",
            "Processing layer conv2_1_1x1_proj\n",
            "Processing layer conv2_1_1x1_proj/bn\n",
            "Processing layer conv2_2_1x1_reduce\n",
            "Processing layer conv2_2_1x1_reduce/bn\n",
            "Processing layer conv2_2_3x3\n",
            "Processing layer conv2_2_3x3/bn\n",
            "Processing layer conv2_2_1x1_increase\n",
            "Processing layer conv2_2_1x1_increase/bn\n",
            "Processing layer conv2_3_1x1_reduce\n",
            "Processing layer conv2_3_1x1_reduce/bn\n",
            "Processing layer conv2_3_3x3\n",
            "Processing layer conv2_3_3x3/bn\n",
            "Processing layer conv2_3_1x1_increase\n",
            "Processing layer conv2_3_1x1_increase/bn\n",
            "Processing layer conv3_1_1x1_reduce\n",
            "Processing layer conv3_1_1x1_reduce/bn\n",
            "Processing layer conv3_1_3x3\n",
            "Processing layer conv3_1_3x3/bn\n",
            "Processing layer conv3_1_1x1_increase\n",
            "Processing layer conv3_1_1x1_increase/bn\n",
            "Processing layer conv3_1_1x1_proj\n",
            "Processing layer conv3_1_1x1_proj/bn\n",
            "Processing layer conv3_2_1x1_reduce\n",
            "Processing layer conv3_2_1x1_reduce/bn\n",
            "Processing layer conv3_2_3x3\n",
            "Processing layer conv3_2_3x3/bn\n",
            "Processing layer conv3_2_1x1_increase\n",
            "Processing layer conv3_2_1x1_increase/bn\n",
            "Processing layer conv3_3_1x1_reduce\n",
            "Processing layer conv3_3_1x1_reduce/bn\n",
            "Processing layer conv3_3_3x3\n",
            "Processing layer conv3_3_3x3/bn\n",
            "Processing layer conv3_3_1x1_increase\n",
            "Processing layer conv3_3_1x1_increase/bn\n",
            "Processing layer conv3_4_1x1_reduce\n",
            "Processing layer conv3_4_1x1_reduce/bn\n",
            "Processing layer conv3_4_3x3\n",
            "Processing layer conv3_4_3x3/bn\n",
            "Processing layer conv3_4_1x1_increase\n",
            "Processing layer conv3_4_1x1_increase/bn\n",
            "Processing layer conv4_1_1x1_reduce\n",
            "Processing layer conv4_1_1x1_reduce/bn\n",
            "Processing layer conv4_1_3x3\n",
            "Processing layer conv4_1_3x3/bn\n",
            "Processing layer conv4_1_1x1_increase\n",
            "Processing layer conv4_1_1x1_increase/bn\n",
            "Processing layer conv4_1_1x1_proj\n",
            "Processing layer conv4_1_1x1_proj/bn\n",
            "Processing layer conv4_2_1x1_reduce\n",
            "Processing layer conv4_2_1x1_reduce/bn\n",
            "Processing layer conv4_2_3x3\n",
            "Processing layer conv4_2_3x3/bn\n",
            "Processing layer conv4_2_1x1_increase\n",
            "Processing layer conv4_2_1x1_increase/bn\n",
            "Processing layer conv4_3_1x1_reduce\n",
            "Processing layer conv4_3_1x1_reduce/bn\n",
            "Processing layer conv4_3_3x3\n",
            "Processing layer conv4_3_3x3/bn\n",
            "Processing layer conv4_3_1x1_increase\n",
            "Processing layer conv4_3_1x1_increase/bn\n",
            "Processing layer conv4_4_1x1_reduce\n",
            "Processing layer conv4_4_1x1_reduce/bn\n",
            "Processing layer conv4_4_3x3\n",
            "Processing layer conv4_4_3x3/bn\n",
            "Processing layer conv4_4_1x1_increase\n",
            "Processing layer conv4_4_1x1_increase/bn\n",
            "Processing layer conv4_5_1x1_reduce\n",
            "Processing layer conv4_5_1x1_reduce/bn\n",
            "Processing layer conv4_5_3x3\n",
            "Processing layer conv4_5_3x3/bn\n",
            "Processing layer conv4_5_1x1_increase\n",
            "Processing layer conv4_5_1x1_increase/bn\n",
            "Processing layer conv4_6_1x1_reduce\n",
            "Processing layer conv4_6_1x1_reduce/bn\n",
            "Processing layer conv4_6_3x3\n",
            "Processing layer conv4_6_3x3/bn\n",
            "Processing layer conv4_6_1x1_increase\n",
            "Processing layer conv4_6_1x1_increase/bn\n",
            "Processing layer conv4_7_1x1_reduce\n",
            "Processing layer conv4_7_1x1_reduce/bn\n",
            "Processing layer conv4_7_3x3\n",
            "Processing layer conv4_7_3x3/bn\n",
            "Processing layer conv4_7_1x1_increase\n",
            "Processing layer conv4_7_1x1_increase/bn\n",
            "Processing layer conv4_8_1x1_reduce\n",
            "Processing layer conv4_8_1x1_reduce/bn\n",
            "Processing layer conv4_8_3x3\n",
            "Processing layer conv4_8_3x3/bn\n",
            "Processing layer conv4_8_1x1_increase\n",
            "Processing layer conv4_8_1x1_increase/bn\n",
            "Processing layer conv4_9_1x1_reduce\n",
            "Processing layer conv4_9_1x1_reduce/bn\n",
            "Processing layer conv4_9_3x3\n",
            "Processing layer conv4_9_3x3/bn\n",
            "Processing layer conv4_9_1x1_increase\n",
            "Processing layer conv4_9_1x1_increase/bn\n",
            "Processing layer conv4_10_1x1_reduce\n",
            "Processing layer conv4_10_1x1_reduce/bn\n",
            "Processing layer conv4_10_3x3\n",
            "Processing layer conv4_10_3x3/bn\n",
            "Processing layer conv4_10_1x1_increase\n",
            "Processing layer conv4_10_1x1_increase/bn\n",
            "Processing layer conv4_11_1x1_reduce\n",
            "Processing layer conv4_11_1x1_reduce/bn\n",
            "Processing layer conv4_11_3x3\n",
            "Processing layer conv4_11_3x3/bn\n",
            "Processing layer conv4_11_1x1_increase\n",
            "Processing layer conv4_11_1x1_increase/bn\n",
            "Processing layer conv4_12_1x1_reduce\n",
            "Processing layer conv4_12_1x1_reduce/bn\n",
            "Processing layer conv4_12_3x3\n",
            "Processing layer conv4_12_3x3/bn\n",
            "Processing layer conv4_12_1x1_increase\n",
            "Processing layer conv4_12_1x1_increase/bn\n",
            "Processing layer conv4_13_1x1_reduce\n",
            "Processing layer conv4_13_1x1_reduce/bn\n",
            "Processing layer conv4_13_3x3\n",
            "Processing layer conv4_13_3x3/bn\n",
            "Processing layer conv4_13_1x1_increase\n",
            "Processing layer conv4_13_1x1_increase/bn\n",
            "Processing layer conv4_14_1x1_reduce\n",
            "Processing layer conv4_14_1x1_reduce/bn\n",
            "Processing layer conv4_14_3x3\n",
            "Processing layer conv4_14_3x3/bn\n",
            "Processing layer conv4_14_1x1_increase\n",
            "Processing layer conv4_14_1x1_increase/bn\n",
            "Processing layer conv4_15_1x1_reduce\n",
            "Processing layer conv4_15_1x1_reduce/bn\n",
            "Processing layer conv4_15_3x3\n",
            "Processing layer conv4_15_3x3/bn\n",
            "Processing layer conv4_15_1x1_increase\n",
            "Processing layer conv4_15_1x1_increase/bn\n",
            "Processing layer conv4_16_1x1_reduce\n",
            "Processing layer conv4_16_1x1_reduce/bn\n",
            "Processing layer conv4_16_3x3\n",
            "Processing layer conv4_16_3x3/bn\n",
            "Processing layer conv4_16_1x1_increase\n",
            "Processing layer conv4_16_1x1_increase/bn\n",
            "Processing layer conv4_17_1x1_reduce\n",
            "Processing layer conv4_17_1x1_reduce/bn\n",
            "Processing layer conv4_17_3x3\n",
            "Processing layer conv4_17_3x3/bn\n",
            "Processing layer conv4_17_1x1_increase\n",
            "Processing layer conv4_17_1x1_increase/bn\n",
            "Processing layer conv4_18_1x1_reduce\n",
            "Processing layer conv4_18_1x1_reduce/bn\n",
            "Processing layer conv4_18_3x3\n",
            "Processing layer conv4_18_3x3/bn\n",
            "Processing layer conv4_18_1x1_increase\n",
            "Processing layer conv4_18_1x1_increase/bn\n",
            "Processing layer conv4_19_1x1_reduce\n",
            "Processing layer conv4_19_1x1_reduce/bn\n",
            "Processing layer conv4_19_3x3\n",
            "Processing layer conv4_19_3x3/bn\n",
            "Processing layer conv4_19_1x1_increase\n",
            "Processing layer conv4_19_1x1_increase/bn\n",
            "Processing layer conv4_20_1x1_reduce\n",
            "Processing layer conv4_20_1x1_reduce/bn\n",
            "Processing layer conv4_20_3x3\n",
            "Processing layer conv4_20_3x3/bn\n",
            "Processing layer conv4_20_1x1_increase\n",
            "Processing layer conv4_20_1x1_increase/bn\n",
            "Processing layer conv4_21_1x1_reduce\n",
            "Processing layer conv4_21_1x1_reduce/bn\n",
            "Processing layer conv4_21_3x3\n",
            "Processing layer conv4_21_3x3/bn\n",
            "Processing layer conv4_21_1x1_increase\n",
            "Processing layer conv4_21_1x1_increase/bn\n",
            "Processing layer conv4_22_1x1_reduce\n",
            "Processing layer conv4_22_1x1_reduce/bn\n",
            "Processing layer conv4_22_3x3\n",
            "Processing layer conv4_22_3x3/bn\n",
            "Processing layer conv4_22_1x1_increase\n",
            "Processing layer conv4_22_1x1_increase/bn\n",
            "Processing layer conv4_23_1x1_reduce\n",
            "Processing layer conv4_23_1x1_reduce/bn\n",
            "Processing layer conv4_23_3x3\n",
            "Processing layer conv4_23_3x3/bn\n",
            "Processing layer conv4_23_1x1_increase\n",
            "Processing layer conv4_23_1x1_increase/bn\n",
            "Processing layer conv5_1_1x1_reduce\n",
            "Processing layer conv5_1_1x1_reduce/bn\n",
            "Processing layer conv5_1_3x3\n",
            "Processing layer conv5_1_3x3/bn\n",
            "Processing layer conv5_1_1x1_increase\n",
            "Processing layer conv5_1_1x1_increase/bn\n",
            "Processing layer conv5_1_1x1_proj\n",
            "Processing layer conv5_1_1x1_proj/bn\n",
            "Processing layer conv5_2_1x1_reduce\n",
            "Processing layer conv5_2_1x1_reduce/bn\n",
            "Processing layer conv5_2_3x3\n",
            "Processing layer conv5_2_3x3/bn\n",
            "Processing layer conv5_2_1x1_increase\n",
            "Processing layer conv5_2_1x1_increase/bn\n",
            "Processing layer conv5_3_1x1_reduce\n",
            "Processing layer conv5_3_1x1_reduce/bn\n",
            "Processing layer conv5_3_3x3\n",
            "Processing layer conv5_3_3x3/bn\n",
            "Processing layer conv5_3_1x1_increase\n",
            "Processing layer conv5_3_1x1_increase/bn\n",
            "Processing layer conv5_3_pool1_conv\n",
            "Processing layer conv5_3_pool1_conv/bn\n",
            "Processing layer conv5_3_pool2_conv\n",
            "Processing layer conv5_3_pool2_conv/bn\n",
            "Processing layer conv5_3_pool3_conv\n",
            "Processing layer conv5_3_pool3_conv/bn\n",
            "Processing layer conv5_3_pool6_conv\n",
            "Processing layer conv5_3_pool6_conv/bn\n",
            "Processing layer conv5_4\n",
            "Processing layer conv5_4/bn\n",
            "Processing layer conv6\n",
            "Processing layer conv4_24\n",
            "Processing layer conv4_24/bn\n",
            "Processing layer conv6_1\n",
            "CONV conv1_1_3x3_s2: Original [64  3  3  3] and trans weights (1728,)\n",
            "BN conv1_1_3x3_s2: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv1_2_3x3: Original [64 64  3  3] and trans weights (36864,)\n",
            "BN conv1_2_3x3: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv1_3_3x3: Original [128  64   3   3] and trans weights (73728,)\n",
            "BN conv1_3_3x3: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv5_3_pool6_conv: Original [ 512 2048    1    1] and trans weights (1048576,)\n",
            "BN conv5_3_pool6_conv: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_3_pool3_conv: Original [ 512 2048    1    1] and trans weights (1048576,)\n",
            "BN conv5_3_pool3_conv: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_3_pool2_conv: Original [ 512 2048    1    1] and trans weights (1048576,)\n",
            "BN conv5_3_pool2_conv: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_3_pool1_conv: Original [ 512 2048    1    1] and trans weights (1048576,)\n",
            "BN conv5_3_pool1_conv: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_4: Original [ 512 4096    3    3] and trans weights (18874368,)\n",
            "BN conv5_4: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv4_24: Original [ 256 1024    3    3] and trans weights (2359296,)\n",
            "BN conv4_24: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv6: Original [ 19 512   1   1] and trans weights (9728,)\n",
            "CONV conv6: Original [19] and trans bias (19,)\n",
            "CONV conv6_1: Original [ 19 256   1   1] and trans weights (4864,)\n",
            "CONV conv6_1: Original [19] and trans bias (19,)\n",
            "CONV conv2_1_1x1_reduce: Original [ 64 128   1   1] and trans weights (8192,)\n",
            "BN conv2_1_1x1_reduce: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv2_1_3x3: Original [64 64  3  3] and trans weights (36864,)\n",
            "BN conv2_1_3x3: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv2_1_1x1_proj: Original [256 128   1   1] and trans weights (32768,)\n",
            "BN conv2_1_1x1_proj: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv2_1_1x1_increase: Original [256  64   1   1] and trans weights (16384,)\n",
            "BN conv2_1_1x1_increase: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv2_2_1x1_reduce: Original [ 64 256   1   1] and trans weights (16384,)\n",
            "BN conv2_2_1x1_reduce: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv2_2_3x3: Original [64 64  3  3] and trans weights (36864,)\n",
            "BN conv2_2_3x3: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv2_2_1x1_increase: Original [256  64   1   1] and trans weights (16384,)\n",
            "BN conv2_2_1x1_increase: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv2_3_1x1_reduce: Original [ 64 256   1   1] and trans weights (16384,)\n",
            "BN conv2_3_1x1_reduce: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv2_3_3x3: Original [64 64  3  3] and trans weights (36864,)\n",
            "BN conv2_3_3x3: Original torch.Size([64]) and trans weights (64,)\n",
            "CONV conv2_3_1x1_increase: Original [256  64   1   1] and trans weights (16384,)\n",
            "BN conv2_3_1x1_increase: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv3_1_1x1_reduce: Original [128 256   1   1] and trans weights (32768,)\n",
            "BN conv3_1_1x1_reduce: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_1_3x3: Original [128 128   3   3] and trans weights (147456,)\n",
            "BN conv3_1_3x3: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_1_1x1_proj: Original [512 256   1   1] and trans weights (131072,)\n",
            "BN conv3_1_1x1_proj: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv3_1_1x1_increase: Original [512 128   1   1] and trans weights (65536,)\n",
            "BN conv3_1_1x1_increase: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv3_2_1x1_reduce: Original [128 512   1   1] and trans weights (65536,)\n",
            "BN conv3_2_1x1_reduce: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_2_3x3: Original [128 128   3   3] and trans weights (147456,)\n",
            "BN conv3_2_3x3: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_2_1x1_increase: Original [512 128   1   1] and trans weights (65536,)\n",
            "BN conv3_2_1x1_increase: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv3_3_1x1_reduce: Original [128 512   1   1] and trans weights (65536,)\n",
            "BN conv3_3_1x1_reduce: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_3_3x3: Original [128 128   3   3] and trans weights (147456,)\n",
            "BN conv3_3_3x3: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_3_1x1_increase: Original [512 128   1   1] and trans weights (65536,)\n",
            "BN conv3_3_1x1_increase: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv3_4_1x1_reduce: Original [128 512   1   1] and trans weights (65536,)\n",
            "BN conv3_4_1x1_reduce: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_4_3x3: Original [128 128   3   3] and trans weights (147456,)\n",
            "BN conv3_4_3x3: Original torch.Size([128]) and trans weights (128,)\n",
            "CONV conv3_4_1x1_increase: Original [512 128   1   1] and trans weights (65536,)\n",
            "BN conv3_4_1x1_increase: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv4_1_1x1_reduce: Original [256 512   1   1] and trans weights (131072,)\n",
            "BN conv4_1_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_1_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_1_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_1_1x1_proj: Original [1024  512    1    1] and trans weights (524288,)\n",
            "BN conv4_1_1x1_proj: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_1_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_1_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_2_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_2_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_2_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_2_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_2_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_2_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_3_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_3_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_3_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_3_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_3_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_3_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_4_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_4_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_4_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_4_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_4_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_4_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_5_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_5_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_5_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_5_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_5_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_5_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_6_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_6_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_6_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_6_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_6_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_6_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_7_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_7_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_7_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_7_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_7_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_7_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_8_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_8_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_8_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_8_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_8_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_8_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_9_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_9_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_9_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_9_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_9_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_9_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_10_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_10_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_10_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_10_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_10_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_10_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_11_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_11_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_11_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_11_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_11_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_11_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_12_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_12_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_12_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_12_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_12_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_12_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_13_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_13_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_13_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_13_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_13_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_13_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_14_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_14_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_14_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_14_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_14_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_14_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_15_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_15_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_15_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_15_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_15_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_15_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_16_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_16_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_16_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_16_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_16_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_16_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_17_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_17_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_17_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_17_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_17_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_17_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_18_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_18_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_18_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_18_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_18_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_18_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_19_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_19_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_19_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_19_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_19_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_19_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_20_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_20_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_20_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_20_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_20_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_20_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_21_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_21_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_21_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_21_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_21_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_21_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_22_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_22_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_22_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_22_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_22_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_22_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv4_23_1x1_reduce: Original [ 256 1024    1    1] and trans weights (262144,)\n",
            "BN conv4_23_1x1_reduce: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_23_3x3: Original [256 256   3   3] and trans weights (589824,)\n",
            "BN conv4_23_3x3: Original torch.Size([256]) and trans weights (256,)\n",
            "CONV conv4_23_1x1_increase: Original [1024  256    1    1] and trans weights (262144,)\n",
            "BN conv4_23_1x1_increase: Original torch.Size([1024]) and trans weights (1024,)\n",
            "CONV conv5_1_1x1_reduce: Original [ 512 1024    1    1] and trans weights (524288,)\n",
            "BN conv5_1_1x1_reduce: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_1_3x3: Original [512 512   3   3] and trans weights (2359296,)\n",
            "BN conv5_1_3x3: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_1_1x1_proj: Original [2048 1024    1    1] and trans weights (2097152,)\n",
            "BN conv5_1_1x1_proj: Original torch.Size([2048]) and trans weights (2048,)\n",
            "CONV conv5_1_1x1_increase: Original [2048  512    1    1] and trans weights (1048576,)\n",
            "BN conv5_1_1x1_increase: Original torch.Size([2048]) and trans weights (2048,)\n",
            "CONV conv5_2_1x1_reduce: Original [ 512 2048    1    1] and trans weights (1048576,)\n",
            "BN conv5_2_1x1_reduce: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_2_3x3: Original [512 512   3   3] and trans weights (2359296,)\n",
            "BN conv5_2_3x3: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_2_1x1_increase: Original [2048  512    1    1] and trans weights (1048576,)\n",
            "BN conv5_2_1x1_increase: Original torch.Size([2048]) and trans weights (2048,)\n",
            "CONV conv5_3_1x1_reduce: Original [ 512 2048    1    1] and trans weights (1048576,)\n",
            "BN conv5_3_1x1_reduce: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_3_3x3: Original [512 512   3   3] and trans weights (2359296,)\n",
            "BN conv5_3_3x3: Original torch.Size([512]) and trans weights (512,)\n",
            "CONV conv5_3_1x1_increase: Original [2048  512    1    1] and trans weights (1048576,)\n",
            "BN conv5_3_1x1_increase: Original torch.Size([2048]) and trans weights (2048,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:287: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:289: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "file {f} saved at /content/drive/My Drive/cityscapes/model/PSP/RESULTS/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzVo0MiRXHBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}